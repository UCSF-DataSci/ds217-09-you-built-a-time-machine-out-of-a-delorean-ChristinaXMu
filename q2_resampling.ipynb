{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fcd9864b",
   "metadata": {},
   "source": [
    "# Question 2: Resampling and Frequency Conversion\n",
    "\n",
    "This question focuses on resampling operations and frequency conversion using ICU monitoring data (hourly) and patient vital signs data (daily).\n",
    "\n",
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5d645281",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "import os\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('default')\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "# Create output directory\n",
    "os.makedirs('output', exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a454b02",
   "metadata": {},
   "source": [
    "## Part 2.1: Load and Prepare Data\n",
    "\n",
    "**Note:** These datasets have realistic characteristics:\n",
    "- **ICU Monitoring**: 75 patients with variable stay lengths (2-30 days). Not all patients are present for the entire 6-month period - patients are admitted and discharged at different times.\n",
    "- **Patient Vitals**: Already contains some missing visits (~5% missing data). This is realistic and will be useful for practicing missing data handling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b166d302",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ICU monitoring shape: (27027, 7)\n",
      "Patient vitals shape: (63624, 7)\n",
      "\n",
      "ICU monitoring sample:\n",
      "                    patient_id  heart_rate  blood_pressure_systolic  \\\n",
      "datetime                                                              \n",
      "2023-03-26 00:00:00     ICU001   75.487292               119.658194   \n",
      "2023-03-26 01:00:00     ICU001   72.404404               109.222567   \n",
      "2023-03-26 02:00:00     ICU001   83.172760               109.448507   \n",
      "2023-03-26 03:00:00     ICU001   86.241612               119.389839   \n",
      "2023-03-26 04:00:00     ICU001   74.536290               119.072058   \n",
      "\n",
      "                     blood_pressure_diastolic  oxygen_saturation  temperature  \n",
      "datetime                                                                       \n",
      "2023-03-26 00:00:00                 83.260736         100.000000         98.1  \n",
      "2023-03-26 01:00:00                 75.955797          96.944358         98.3  \n",
      "2023-03-26 02:00:00                 76.113955          98.137873         98.0  \n",
      "2023-03-26 03:00:00                 83.072887          97.902540         98.6  \n",
      "2023-03-26 04:00:00                 82.850440          97.731986         98.3  \n",
      "\n",
      "Patient vitals sample:\n",
      "           patient_id  temperature  heart_rate  blood_pressure_systolic  \\\n",
      "date                                                                      \n",
      "2023-02-22      P0001         98.4   84.018315                      125   \n",
      "2023-02-23      P0001         97.7   81.152032                      131   \n",
      "2023-02-24      P0001         98.8   88.550207                      131   \n",
      "2023-02-26      P0001         99.0   87.471654                      127   \n",
      "2023-02-27      P0001         98.3   84.540929                      130   \n",
      "\n",
      "            blood_pressure_diastolic  weight  \n",
      "date                                          \n",
      "2023-02-22                        83    68.9  \n",
      "2023-02-23                        89    69.0  \n",
      "2023-02-24                        89    67.8  \n",
      "2023-02-26                        85    69.6  \n",
      "2023-02-27                        88    68.2  \n",
      "\n",
      "ICU patients: 75\n",
      "ICU date range: 2023-01-01 00:00:00 to 2023-05-23 00:00:00\n",
      "\n",
      "Patient vitals patients: 200\n",
      "Patient vitals date range: 2023-01-01 00:00:00 to 2023-12-31 00:00:00\n"
     ]
    }
   ],
   "source": [
    "# Load ICU monitoring data (hourly)\n",
    "icu_monitoring = pd.read_csv('data/icu_monitoring.csv')\n",
    "\n",
    "# Load patient vitals data (daily) - for comparison\n",
    "patient_vitals = pd.read_csv('data/patient_vitals.csv')\n",
    "\n",
    "print(\"ICU monitoring shape:\", icu_monitoring.shape)\n",
    "print(\"Patient vitals shape:\", patient_vitals.shape)\n",
    "\n",
    "# Convert datetime columns and set as index\n",
    "icu_monitoring['datetime'] = pd.to_datetime(icu_monitoring['datetime'])\n",
    "icu_monitoring = icu_monitoring.set_index('datetime')\n",
    "\n",
    "patient_vitals['date'] = pd.to_datetime(patient_vitals['date'])\n",
    "patient_vitals = patient_vitals.set_index('date')\n",
    "\n",
    "print(\"\\nICU monitoring sample:\")\n",
    "print(icu_monitoring.head())\n",
    "print(\"\\nPatient vitals sample:\")\n",
    "print(patient_vitals.head())\n",
    "\n",
    "# Check data characteristics\n",
    "print(f\"\\nICU patients: {icu_monitoring['patient_id'].nunique()}\")\n",
    "print(f\"ICU date range: {icu_monitoring.index.min()} to {icu_monitoring.index.max()}\")\n",
    "print(f\"\\nPatient vitals patients: {patient_vitals['patient_id'].nunique()}\")\n",
    "print(f\"Patient vitals date range: {patient_vitals.index.min()} to {patient_vitals.index.max()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ca7dd39",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "## Part 2.2: Time Series Selection\n",
    "\n",
    "**‚ö†Ô∏è WARNING: Sort Index Before Date Selection!**\n",
    "Since multiple patients share the same date, the `patient_vitals` index is non-monotonic (not strictly increasing). **You MUST sort the index first** before using `.loc` with date ranges:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4f905616",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "patient_vitals = patient_vitals.sort_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "551db0cf",
   "metadata": {},
   "source": [
    "Without sorting, pandas cannot reliably handle date range selections and may return unexpected results or errors.\n",
    "\n",
    "**TODO: Perform time series indexing and selection**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4f8e2fa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "January 1, 2023 data: patient_id                     P0060\n",
      "temperature                     98.6\n",
      "heart_rate                  82.47421\n",
      "blood_pressure_systolic          111\n",
      "blood_pressure_diastolic          76\n",
      "weight                          72.7\n",
      "Name: 2023-01-01 00:00:00, dtype: object\n",
      "Records on Jan 1: 6 (some patients may start later)\n",
      "January 2023 shape: (1446, 6)\n",
      "\n",
      "First quarter average temperature: 98.87¬∞F\n",
      "After June average temperature: 98.23¬∞F\n",
      "First week average temperature: 98.62¬∞F\n",
      "Last week average temperature: 98.46¬∞F\n",
      "Business hours data shape: (10107, 6)\n",
      "\n",
      "Average heart rate - All hours: 81.2 bpm\n",
      "Average heart rate - Business hours: 80.0 bpm\n",
      "Average temperature - All hours: 99.0¬∞F\n",
      "Average temperature - Business hours: 99.0¬∞F\n"
     ]
    }
   ],
   "source": [
    "# TODO: Select data by specific dates\n",
    "# Note: Not all patients may have data on January 1, 2023 (some start later)\n",
    "# Important: Sort the index first since multiple patients share the same date\n",
    "patient_vitals = patient_vitals.sort_index()  # Sort for reliable date-based selection\n",
    "# Select January 1, 2023 from patient_vitals\n",
    "january_first = patient_vitals.loc['2023-01-01']\n",
    "print(\"January 1, 2023 data:\", january_first)\n",
    "print(f\"Records on Jan 1: {len(january_first)} (some patients may start later)\")\n",
    "\n",
    "# TODO: Select data by date ranges\n",
    "# Select entire January 2023\n",
    "january_data = patient_vitals.loc['2023-01-01':'2023-01-31']  \n",
    "print(\"January 2023 shape:\", january_data.shape)\n",
    "\n",
    "# TODO: Select data by time periods\n",
    "first_quarter = patient_vitals.loc['2023-01-01':'2023-03-31']  # Select Q1 2023\n",
    "entire_year = patient_vitals.loc['2023']  # Select all of 2023 (will include patients with partial year data)\n",
    "\n",
    "# TODO: Select first and last periods using .loc\n",
    "first_week = patient_vitals.loc[:patient_vitals.index.min() + pd.Timedelta(days=6)]  # First 7 days\n",
    "last_week = patient_vitals.loc[patient_vitals.index.max() - pd.Timedelta(days=6):]  # Last 7 days\n",
    "\n",
    "# TODO: Use truncate() method\n",
    "# Note: truncate() requires a sorted index. Sort first if needed: patient_vitals = patient_vitals.sort_index()\n",
    "data_after_june = patient_vitals.truncate(before='2023-06-01')  # Truncate before June 1, 2023\n",
    "data_before_september = patient_vitals.truncate(after='2023-08-31')   # Truncate after August 31, 2023\n",
    "\n",
    "# TODO: Use selected data for analysis\n",
    "# Compare average temperature between first quarter and data after June\n",
    "print(f\"\\nFirst quarter average temperature: {first_quarter['temperature'].mean():.2f}¬∞F\")\n",
    "print(f\"After June average temperature: {data_after_june['temperature'].mean():.2f}¬∞F\")\n",
    "print(f\"First week average temperature: {first_week['temperature'].mean():.2f}¬∞F\")\n",
    "print(f\"Last week average temperature: {last_week['temperature'].mean():.2f}¬∞F\")\n",
    "\n",
    "# For ICU data with time components:\n",
    "# TODO: Select business hours (9 AM to 5 PM)\n",
    "business_hours = icu_monitoring.between_time('09:00', '17:00')  # Use between_time()\n",
    "print(\"Business hours data shape:\", business_hours.shape)\n",
    "\n",
    "# TODO: Select specific time (noon readings)\n",
    "noon_data = icu_monitoring.at_time('12:00')  # Use at_time('12:00')\n",
    "\n",
    "# TODO: Use time-based selection for analysis\n",
    "# Compare vital signs during business hours vs other times\n",
    "all_hours_avg = icu_monitoring.select_dtypes(include=[np.number]).mean()\n",
    "business_hours_avg = business_hours.select_dtypes(include=[np.number]).mean()\n",
    "print(f\"\\nAverage heart rate - All hours: {all_hours_avg['heart_rate']:.1f} bpm\")\n",
    "print(f\"Average heart rate - Business hours: {business_hours_avg['heart_rate']:.1f} bpm\")\n",
    "print(f\"Average temperature - All hours: {all_hours_avg['temperature']:.1f}¬∞F\")\n",
    "print(f\"Average temperature - Business hours: {business_hours_avg['temperature']:.1f}¬∞F\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4653f2fe",
   "metadata": {},
   "source": [
    "## Part 2.3: Resampling Operations\n",
    "\n",
    "**TODO: Perform resampling and frequency conversion**\n",
    "\n",
    "**Important Note:** When resampling DataFrames that contain non-numeric columns (like `patient_id`), you'll get an error if you try to aggregate them with numeric functions like `mean()`. Use `df.select_dtypes(include=[np.number])` to select only numeric columns before resampling, or specify which columns to aggregate in `.agg()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d18466a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ICU daily shape: (143, 5)\n",
      "Weekly resampled shape: (53, 5)\n",
      "Monthly resampled shape: (12, 5)\n",
      "ICU daily stats shape: (143, 20)\n",
      "Missing values after upsampling from monthly to daily:\n",
      "Missing values after upsampling: temperature                 323\n",
      "heart_rate                  323\n",
      "blood_pressure_systolic     323\n",
      "blood_pressure_diastolic    323\n",
      "weight                      323\n",
      "dtype: int64\n",
      "Aggregated daily patient vitals shape: (365, 5)\n",
      "\n",
      "Resampling comparison:\n",
      "  frequency                date_range  row_count  mean_temperature  \\\n",
      "0     daily  2023-01-01 to 2023-12-31        365         98.481643   \n",
      "1    weekly  2023-01-01 to 2023-12-31         53         98.484639   \n",
      "2   monthly  2023-01-31 to 2023-12-31         12         98.486513   \n",
      "\n",
      "   std_temperature  \n",
      "0         0.354911  \n",
      "1         0.352725  \n",
      "2         0.364940  \n",
      "\n",
      "Resampling analysis saved to 'output/q2_resampling_analysis.csv'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/6w/gv8g6zzd417_9g9fl64d_nhc0000gn/T/ipykernel_66766/1828878904.py:16: FutureWarning: 'M' is deprecated and will be removed in a future version, please use 'ME' instead.\n",
      "  patient_vitals_monthly = patient_vitals[numeric_cols_pv].resample('M').mean() # Resample to monthly with mean aggregation (use freq='ME' for Month End)\n"
     ]
    }
   ],
   "source": [
    "# TODO: Resample hourly ICU data to daily\n",
    "# Note: Exclude non-numeric columns like 'patient_id' when resampling\n",
    "# Select only numeric columns before resampling\n",
    "numeric_cols = icu_monitoring.select_dtypes(include=[np.number]).columns\n",
    "icu_daily = icu_monitoring[numeric_cols].resample('D').mean()\n",
    "print(\"ICU daily shape:\", icu_daily.shape)\n",
    "\n",
    "# TODO: Resample daily patient data to weekly\n",
    "# Note: Exclude 'patient_id' column when resampling\n",
    "# Select only numeric columns before resampling\n",
    "numeric_cols_pv = patient_vitals.select_dtypes(include=[np.number]).columns\n",
    "patient_vitals_weekly = patient_vitals[numeric_cols_pv].resample('W').mean()\n",
    "print(\"Weekly resampled shape:\", patient_vitals_weekly.shape)\n",
    "\n",
    "# TODO: Resample daily patient data to monthly\n",
    "patient_vitals_monthly = patient_vitals[numeric_cols_pv].resample('M').mean() # Resample to monthly with mean aggregation (use freq='ME' for Month End)\n",
    "print(\"Monthly resampled shape:\", patient_vitals_monthly.shape)\n",
    "\n",
    "# TODO: Use different aggregation functions (mean, sum, max, min)\n",
    "icu_daily_stats = icu_monitoring[numeric_cols].resample('D').agg(['mean', 'sum', 'max', 'min'])\n",
    "print(\"ICU daily stats shape:\", icu_daily_stats.shape)  \n",
    "# Resample with multiple aggregations\n",
    "# Example: resample('D').agg({'heart_rate': ['mean', 'max', 'min'], \n",
    "#                             'temperature': 'mean'})\n",
    "\n",
    "# TODO: Handle missing values during resampling\n",
    "# Demonstrate upsampling (monthly to daily) creates missing values\n",
    "# Note: When upsampling, use .asfreq() to create missing values, or use .resample() with aggregation\n",
    "monthly_to_daily = patient_vitals_monthly.resample('D').asfreq() # Upsample monthly data to daily (use .asfreq() or .resample('D'))\n",
    "print(\"Missing values after upsampling from monthly to daily:\")\n",
    "print(\"Missing values after upsampling:\", monthly_to_daily.isna().sum())\n",
    "\n",
    "# TODO: Compare different resampling frequencies\n",
    "# Create a DataFrame comparing resampling results at different frequencies\n",
    "# Important: Since patient_vitals contains multiple patients per date, you need to aggregate by date first\n",
    "# to create a single daily time series for comparison.\n",
    "# Why aggregation is needed: The patient_vitals DataFrame has multiple rows per date (one for each patient),\n",
    "# so we need to average across patients for each date to create a single daily time series that can be\n",
    "# meaningfully compared with the weekly and monthly resampled data. Without aggregation, resampling would\n",
    "# operate on each patient's time series separately, making it difficult to compare frequencies meaningfully.\n",
    "# Steps:\n",
    "# 1. Since 'date' is currently the index, reset it to a column first, then aggregate by date\n",
    "#    Note: groupby('date').mean() automatically sets 'date' as the index in the result, so you don't need\n",
    "#    to call set_index('date') again after groupby.\n",
    "patient_vitals_reset = patient_vitals[numeric_cols_pv].reset_index()\n",
    "patient_vitals_daily_agg = patient_vitals_reset.groupby('date').mean()\n",
    "print(\"Aggregated daily patient vitals shape:\", patient_vitals_daily_agg.shape)\n",
    "#    # The date is already the index after groupby, so no need to set_index again\n",
    "# 2. Compare the aggregated daily data with weekly and monthly resampled data\n",
    "# Use patient_vitals data resampled to different frequencies:\n",
    "# - Original daily data (aggregated by date): patient_vitals_daily_agg\n",
    "# - Weekly resampled (patient_vitals_weekly) \n",
    "# - Monthly resampled (patient_vitals_monthly)\n",
    "# Include columns: frequency, date_range, row_count, mean_temperature, std_temperature\n",
    "# Use the 'temperature' column from each resampled dataset\n",
    "# Example structure:\n",
    "resampling_comparison = pd.DataFrame({\n",
    "    'frequency': ['daily', 'weekly', 'monthly'],\n",
    "    'date_range': [\n",
    "        f\"{patient_vitals_daily_agg.index.min().date()} to {patient_vitals_daily_agg.index.max().date()}\",\n",
    "        f\"{patient_vitals_weekly.index.min().date()} to {patient_vitals_weekly.index.max().date()}\",\n",
    "        f\"{patient_vitals_monthly.index.min().date()} to {patient_vitals_monthly.index.max().date()}\"\n",
    "    ],\n",
    "    'row_count': [\n",
    "        len(patient_vitals_daily_agg),\n",
    "        len(patient_vitals_weekly),\n",
    "        len(patient_vitals_monthly)\n",
    "    ],\n",
    "    'mean_temperature': [\n",
    "        patient_vitals_daily_agg['temperature'].mean(),\n",
    "        patient_vitals_weekly['temperature'].mean(),\n",
    "        patient_vitals_monthly['temperature'].mean()\n",
    "    ],\n",
    "    'std_temperature': [\n",
    "        patient_vitals_daily_agg['temperature'].std(),\n",
    "        patient_vitals_weekly['temperature'].std(),\n",
    "        patient_vitals_monthly['temperature'].std()\n",
    "    ]\n",
    "})\n",
    "\n",
    "print(\"\\nResampling comparison:\")\n",
    "print(resampling_comparison)\n",
    "\n",
    "# TODO: Save results as 'output/q2_resampling_analysis.csv'\n",
    "resampling_comparison.to_csv('output/q2_resampling_analysis.csv', index=False)\n",
    "print(\"\\nResampling analysis saved to 'output/q2_resampling_analysis.csv'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3605486d",
   "metadata": {},
   "source": [
    "## Part 2.4: Missing Data Handling\n",
    "\n",
    "**üí° TIP: High Percentage of Missing Data is Expected!**\n",
    "When upsampling from monthly to daily frequency, you'll create approximately 96% missing data (only 12 month-end dates have values out of 365 days). This is normal and expected for upsampling - don't be alarmed!\n",
    "\n",
    "**Approach:** Create missing values by upsampling monthly data to daily frequency. This creates a clear, structured pattern of missing data that's ideal for practicing imputation methods.\n",
    "\n",
    "**TODO: Handle missing data in time series**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9dd87022",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing value count: 323\n",
      "Missing value percentage: 96.41791044776119\n"
     ]
    }
   ],
   "source": [
    "# TODO: Identify missing values in time series\n",
    "# Use the monthly resampled data from Part 2.3 and upsample to daily:\n",
    "#   - Take patient_vitals_monthly['temperature']\n",
    "#   - Upsample to daily frequency using .resample('D').asfreq()\n",
    "#   - This creates missing values for all days except month-end dates (~96% missing)\n",
    "ts_with_missing = patient_vitals_monthly['temperature'].resample('D').asfreq()  # Time series with missing values  \n",
    "print(\"Missing value count:\", ts_with_missing.isna().sum())\n",
    "print(\"Missing value percentage:\", ts_with_missing.isna().sum() / len(ts_with_missing) * 100)\n",
    "\n",
    "# TODO: Use forward fill and backward fill\n",
    "ts_ffill = ts_with_missing.ffill()  # Forward fill missing values (use .ffill() method)\n",
    "ts_bfill = ts_with_missing.bfill()  # Backward fill missing values (use .bfill() method)\n",
    "\n",
    "# TODO: Use interpolation methods\n",
    "ts_interpolated = ts_with_missing.interpolate() # Interpolate missing values\n",
    "ts_interpolated_linear = ts_with_missing.interpolate(method='linear')  # Linear interpolation\n",
    "ts_interpolated_time = ts_with_missing.interpolate(method='time')  # Time-based interpolation\n",
    "\n",
    "# TODO: Use rolling mean for imputation\n",
    "ts_rolling_imputed = ts_with_missing.rolling(window=3, min_periods=1).mean()  # Fill missing with rolling mean\n",
    "\n",
    "# TODO: Create missing data report\n",
    "# Document your missing data handling with the following sections:\n",
    "# 1. Missing value summary: Total count and percentage\n",
    "# 2. Missing data patterns: When/why data is missing (by month, day of week, etc.)\n",
    "# 3. Imputation method: Which method you used (forward fill, backward fill, interpolation, rolling mean)\n",
    "# 4. Rationale: Why you chose that method\n",
    "# 5. Pros and cons: Advantages and limitations of your approach\n",
    "# 6. Example: Show at least one example of missing data before and after imputation\n",
    "# Minimum length: 300 words\n",
    "\n",
    "# TODO: Document missing data patterns\n",
    "# Analyze when/why data is missing\n",
    "missing_by_month = ts_with_missing.groupby(ts_with_missing.index.month).apply(lambda x: x.isna().sum())\n",
    "missing_by_day = ts_with_missing.groupby(ts_with_missing.index.dayofweek).apply(lambda x: x.isna().sum())\n",
    "missing_patterns = f\"Missing by month:\\n{missing_by_month}\\n\\nMissing by day of week:\\n{missing_by_day}\"\n",
    "\n",
    "missing_data_report = f\"\"\"\n",
    "\n",
    "1) Missing value summary:\n",
    "Total missing values: {ts_with_missing.isna().sum()}\n",
    "Percentage missing: {ts_with_missing.isna().sum() / len(ts_with_missing) * 100:.2f}%\n",
    "\n",
    "- There are 323 missing values and the missing value percentage is 96.4%.\n",
    "\n",
    "2) Imputation methods applied for handling missing data:\n",
    "- Forward fill (ts_ffill)\n",
    "- Backward fill (ts_bfill)\n",
    "- Linear interpolation (ts_interpolated_linear)\n",
    "- Time-based interpolation (ts_interpolated_time)\n",
    "- Rolling mean imputation with 3-day window (ts_rolling_imputed)\n",
    "\n",
    "3) Why did you choose that method?\n",
    "- Forward/backward fill maintains last known value and is simple for short gaps.\n",
    "- Interpolation estimates missing values based on surrounding known points.\n",
    "- Rolling mean smooths out fluctuations and reduces noise in imputed values.\n",
    "\n",
    "4) What are the pros/cons of your approach?\n",
    "- Forward/backward fill: \n",
    "    Pros: Simple and fast approach\n",
    "    Cons: Can lead to biased distribution and errors if there is ectensive missingness. Not a suitable method for long gaps or trends. Backward fill cannot fill the last missing values.\n",
    "- Interpolation: \n",
    "    Pros: Captures trends and better maintains data distribution and relationships between variables\n",
    "    Cons: More complex, may not work well for non-linear trends or high volatility\n",
    "- Rolling mean: \n",
    "    Pros: Reduces noise and smooths imputed values, suitable for time-series data\n",
    "    Cons: More complex, may lag behind actual values, choice of window size affects results, trends to underestimate the true variance\n",
    "\n",
    "5) Examples:\n",
    "Original missing data (first 10 days of January):\n",
    "{ts_with_missing.head(10)}\n",
    "\n",
    "Forward fill imputed values:\n",
    "{ts_ffill.head(10)}\n",
    "\n",
    "Linear interpolation imputed values:\n",
    "{ts_interpolated_linear.head(10)}\n",
    "\"\"\"\n",
    "\n",
    "# TODO: Save results as 'output/q2_missing_data_report.txt'\n",
    "with open('output/q2_missing_data_report.txt', 'w') as f:\n",
    "    f.write(missing_data_report)\n",
    "    f.write(f\"\\n\\nMissing patterns:\\n{missing_patterns}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81cf350d",
   "metadata": {},
   "source": [
    "## Submission Checklist\n",
    "\n",
    "Before moving to Question 3, verify you've created:\n",
    "\n",
    "- [ ] `output/q2_resampling_analysis.csv` - resampling analysis results\n",
    "- [ ] `output/q2_missing_data_report.txt` - missing data handling report\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
